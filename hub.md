# 今後読む論文のリスト

## ML
* LightGBM: A Highly Efficient Gradient Boosting Decision Tree
* XGBoost: A Scalable Tree Boosting System
* Semi-Supervised Segmentation of Salt Bodies in Seismic Images using an Ensemble of Convolutional Neural Networks
## NLP
* ~~BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding~~
* ~~RoBERTa: A Robustly Optimized BERT Pretraining Approach~~
* ~~ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators~~
* DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
* ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
* BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
* DeBERTa: Decoding-enhanced BERT with Disentangled Attention
* MASS: Masked Sequence to Sequence Pre-training for Language Generation
* Unified Language Model Pre-training for Natural Language Understanding and Generation
* DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing
* Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing
* Longformer: The Long-Document Transformer
* Cross-lingual Language Model Pretraining
## V&L
* OCR-free Document Understanding Transformer
* DePlot: One-shot visual language reasoning by plot-to-table translation
## CV
* Deep Residual Learning for Image Recognition
* EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
* EfficientNetV2: Smaller Models and Faster Training
## DML
* SimCSE: Simple Contrastive Learning of Sentence Embeddings
* ArcFace: Additive Angular Margin Loss for Deep Face Recognition