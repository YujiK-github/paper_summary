# 今後読む論文のリスト

## ML
* LightGBM: A Highly Efficient Gradient Boosting Decision Tree
* XGBoost: A Scalable Tree Boosting System
* Semi-Supervised Segmentation of Salt Bodies in Seismic Images using an Ensemble of Convolutional Neural Networks
## NLP
* ~~BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding~~
* ~~RoBERTa: A Robustly Optimized BERT Pretraining Approach~~
* ~~ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators~~
* DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
* ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
* BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
* DeBERTa: Decoding-enhanced BERT with Disentangled Attention
* MASS: Masked Sequence to Sequence Pre-training for Language Generation
* Unified Language Model Pre-training for Natural Language Understanding and Generation
* DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing
* Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing
* Longformer: The Long-Document Transformer
* Cross-lingual Language Model Pretraining
## V&L
* ~~OCR-free Document Understanding Transformer~~
* ~~DePlot: One-shot visual language reasoning by plot-to-table translation~~
* Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
* Self-Consistency Improves Chain of Thought Reasoning in Language Models
## CV
* Deep Residual Learning for Image Recognition
* EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
* EfficientNetV2: Smaller Models and Faster Training
* Learning Transferable Visual Models From Natural Language Supervision(CLIP)
* An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
* MLP-Mixer: An all-MLP Architecture for Vision
* How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers
* When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations
* LiT: Zero-Shot Transfer with Locked-image text Tuning
* Surrogate Gap Minimization Improves Sharpness-Aware Training
* Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
* ChartOCR: Data Extraction from Charts Images via a Deep Hybrid Framework
## DML
* SimCSE: Simple Contrastive Learning of Sentence Embeddings
* ArcFace: Additive Angular Margin Loss for Deep Face Recognition