# 今後読む論文のリスト

## ML
* LightGBM: A Highly Efficient Gradient Boosting Decision Tree
* XGBoost: A Scalable Tree Boosting System
* CatBoost: unbiased boosting with categorical features
* Semi-Supervised Segmentation of Salt Bodies in Seismic Images using an Ensemble of Convolutional Neural Networks

## DL
* Layer Normalization
* Distilling the Knowledge in a Neural Network

## NLP
* ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
* BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
* DeBERTa: Decoding-enhanced BERT with Disentangled Attention
* MASS: Masked Sequence to Sequence Pre-training for Language Generation
* Unified Language Model Pre-training for Natural Language Understanding and Generation
* DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing
* Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing
* Longformer: The Long-Document Transformer
* Cross-lingual Language Model Pretraining
* SpanBERT: Improving Pre-training by Representing and Predicting Spans
* PMI-Masking: Principled masking of correlated spans
* How to Train BERT with an Academic Budget
## V&L
* Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
* Self-Consistency Improves Chain of Thought Reasoning in Language Models
## CV
* Deep Residual Learning for Image Recognition
* EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
* EfficientNetV2: Smaller Models and Faster Training
* Learning Transferable Visual Models From Natural Language Supervision(CLIP)
* An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
* MLP-Mixer: An all-MLP Architecture for Vision
* How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers
* When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations
* LiT: Zero-Shot Transfer with Locked-image text Tuning
* Surrogate Gap Minimization Improves Sharpness-Aware Training
* Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
* ChartOCR: Data Extraction from Charts Images via a Deep Hybrid Framework
## DML
* ArcFace: Additive Angular Margin Loss for Deep Face Recognition
* Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning
* Big Self-Supervised Models are Strong Semi-Supervised Learners(SimCLR ver2)

## Table
* Revisiting Deep Learning Models for Tabular Data
* TabTransformer: Tabular Data Modeling Using Contextual Embeddings
* TabNet: Attentive Interpretable Tabular Learning