{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransTab: Learning Transferable Tabular Transformers Across Tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. どんなもの？\n",
    "Tableデータにおける新しいフレームワークのTransTab, which stands for **Trans**ferable **Trans**formers for **Tab**ular anlysisの提案."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 先行研究と比べるとどこが凄い？  \n",
    "先行研究ではテーブルデータに対して機械学習や深層学習を用いる時に訓練データとテストデータで同じカラムを要求していた。しかしTransTabは特徴量をテキストに変換しそれをtransformerを利用してembeddingsに変換することで同じカラムにとらわれないこと、データを無駄にしないことを可能にした。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 技術や手法のキモはどこ？\n",
    "* inputの特徴量をCategorical/Textual feature, Binary feature, Numerical featureで場合分ける。\n",
    "  * Categorical/Textual feature  \n",
    "  カラム名とvalueをつなげてtokenized and encoded\n",
    "  * Binary feature  \n",
    "  計算量やメモリー削減のためにbinaryが1のときのみtokenized and encoded\n",
    "  * Numerical feature  \n",
    "  tokenizerは数字の識別があまり良くないらしいのでカラム名のみをtokenized and encodedしたものにvalueをかける\n",
    "そうして出来たembeddingsをnormalizationして、同じ空間に同じ線形層を適用し、$E = E_{c} \\bigotimes E_{u}(=E_{u, col^2} * x_{u}) \\bigotimes E_{b} \\bigotimes E^{cls}$とする\n",
    "* EをMulti-head self-attention layerに入力. その出力をtoken-wise gating layer(sigmoid)によって変換しそれぞれのtokenの重要度を求める。線形層を通したMulti-head self-attention layerの出力と、token-wise gating layerを通した出力とMulti-head self-attention layerの積の和をさらに線形層に通す。それを何回か繰り返す。\n",
    "* Self-supervised Vertical-Partition Contrastive Learning  \n",
    "本文の内容が全く分からないが、図を見るに、同じ行のセルをそれぞれembeddings上で近くなるように、異なる行のセルを遠ざけるように設定。\n",
    "* Supervised Vertical-Partition Contrastive Learning  \n",
    "本文の内容が全く分からないが、図を見るに、同じラベルをもつもののembeddingを近づけるようにして、異なるラベルを持つものを遠ざけるようにする"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. どうやって有効だと検証した？\n",
    "dataset: clinical traial mortality prediction datasets and public datasets\n",
    "\n",
    "1. How does TransTab perform compared with baselines under the vanilla supervised setting?  \n",
    "  -> Yes(SOTA without pre-training)\n",
    "2. How well does TransTab address incremental columns from a streeam of data?  \n",
    "  -> Yes(データをいくつかに分割して学習させた)\n",
    "3. How is the impact of TransTab learned from muliple tables drawn from the same domain in its oredctive ability?  \n",
    "   -> Yes(データを50％重複するように分割し片方でpre-train, もう片方でfine-tuning)\n",
    "4. Can TransTab be a zero-shot learner when pretrained on tables and infer on a new table?  \n",
    "   -> Yes(データを三つに分解し、そのうち2つをpre-train、もう一つでZero-shotした)\n",
    "5. Is the proposed vertical partition CL better than vanilla supervised pretraining and self-supervised CL?  \n",
    "   -> Yes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 議論はある？\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 次に読むべき論文は？\n",
    "* Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models(tokenizerによる数字の識別の検証)\n",
    "* Layer Normalization\n",
    "* TabTransformer: Tabular Data Modeling Using Contextual Embeddings\n",
    "* TabNet: Attentive Interpretable Tabular Learning\n",
    "* Revisiting Deep Learning Models for Tabular Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 実装\n",
    "https://github.com/ryanwangzf/transtab"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
