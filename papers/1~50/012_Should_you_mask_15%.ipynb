{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Should You Mask 15% in Masked Language Modeling?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* URL: https://arxiv.org/abs/2202.08005\n",
    "* authors:Alexander Wettig, Tianyu Gao, Zexuan Zhong, Danqi Chen\n",
    "* Submitted on 16 Feb 2022 (v1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. どんなもの？\n",
    "BERTなどで使われてきた15%のmaskingの割合について疑問を呈した論文。\n",
    "\n",
    "## 2. 先行研究と比べるとどこが凄い？  \n",
    "maskingの割合だけではなく、モデルのサイズとの関係やmaskingの方法との関係を調べている。\n",
    "\n",
    "## 3. 技術や手法のキモはどこ？\n",
    "### model sizeとの関係\n",
    "large(354M para): 40%, base(124M para): 25%, medium(51M para): 15%  \n",
    "性能が一定に達する時間も短くなっている。\n",
    "\n",
    "### 高い割合でmaskingを行ったとき(80%)\n",
    "pre-trainingのperplexityは高かったが、fine-tuningのときは95%の性能を得た(何の？)\n",
    "\n",
    "### the strategy of which tokens to mask\n",
    "* uniform masking\n",
    "* span Masking\n",
    "* PMI masking  \n",
    "  \n",
    "で試した。最適な割合は15%よりも大きく、spanとPMIの最適割合はuniformのものよりも小さく、それぞれが最適割合でmaskingを行ったときにはuniformがベストだった。\n",
    "\n",
    "### Masking as corruption and prediction\n",
    "Maskingは2つの見方ができる。masking ratio, m, は文章で崩壊している部分の割合、もしくは文章で予測すべき単語の割合である。この二つの影響を見るために以下の実験を行った。\n",
    "1. $m_{pred} < m_{corr}$\n",
    "   m_{corr}の一部を予測する。\n",
    "2. $m_{pred} > m_{corr}$  \n",
    "   文章を$\\lceil \\frac{m_{pred}}{m_{corr}}\\rceil$(切り上げ)倍だけ増やして、それぞれの文章で異なるトークンをmaskし、全てを予測する。\n",
    "\n",
    "その結果、predictionのマスクの割合を大きくすると改善、corruptionのマスクの割合を大きくすると悪化した。  \n",
    "-> 公式実装で見つけられなかった。どこだろ。\n",
    "\n",
    "### Revisiting BERT's Corruption Strategy\n",
    "random replaceやsame replaceは、全てMASKにreplaceするときに比べてパフォーマンスが悪かった。\n",
    "\n",
    "## 4. どうやって有効だと検証した？\n",
    "24hBERTに以下の変化を加えたモデルを使用。\n",
    "* RoBERTa's BPE tokenizer\n",
    "* BERTの80-10-10のもとでのmaskingの代わりに100で[MASK]に置き換える\n",
    "\n",
    "あまりはっきりと書いてないけどBLUEで検証したのかな。\n",
    "\n",
    "## 5. 議論はある？\n",
    "* ELECTRA-typeやseq2seq-typeでの最適maskingの割合はどのくらいだろう？\n",
    "* 別の言語ではどう？\n",
    "* 実験は複数のseedで行っていないので結果は怪しいかも。  \n",
    "->three seedsでやったって書いてあるな\n",
    "\n",
    "\n",
    "## 6. 次に読むべき論文は？\n",
    "* SpanBERT: Improving Pre-training by Representing and Predicting Spans\n",
    "* PMI-Masking: Principled masking of correlated spans\n",
    "* How to Train BERT with an Academic Budget\n",
    "## 7. 実装\n",
    "https://github.com/princeton-nlp/dinkytrain"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
