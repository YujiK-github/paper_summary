# Should You Mask 15% in Masked Language Modeling?

* URL: https://arxiv.org/abs/2202.08005
* authors:Alexander Wettig, Tianyu Gao, Zexuan Zhong, Danqi Chen
* Submitted on 16 Feb 2022 (v1)

## 1. どんなもの？
BERTなどで使われてきた15%のmaskingの割合について疑問を呈した論文。

## 2. 先行研究と比べるとどこが凄い？  
maskingの割合だけではなく、モデルのサイズとの関係やmaskingの方法との関係を調べている。

## 3. 技術や手法のキモはどこ？
### model sizeとの関係
large(354M para): 40%, base(124M para): 25%, medium(51M para): 15%  
性能が一定に達する時間も短くなっている。

### 高い割合でmaskingを行ったとき(80%)
pre-trainingのperplexityは高かったが、fine-tuningのときは95%の性能を得た(何の？)

### the strategy of which tokens to mask
* uniform masking
* span Masking
* PMI masking  
  
で試した。最適な割合は15%よりも大きく、spanとPMIの最適割合はuniformのものよりも小さく、それぞれが最適割合でmaskingを行ったときにはuniformがベストだった。

### Masking as corruption and prediction
Maskingは2つの見方ができる。masking ratio, m, は文章で崩壊している部分の割合、もしくは文章で予測すべき単語の割合である。この二つの影響を見るために以下の実験を行った。
1. $m_{pred} < m_{corr}$
   m_{corr}の一部を予測する。
2. $m_{pred} > m_{corr}$  
   文章を$\lceil \frac{m_{pred}}{m_{corr}}\rceil$(切り上げ)倍だけ増やして、それぞれの文章で異なるトークンをmaskし、全てを予測する。

その結果、predictionのマスクの割合を大きくすると改善、corruptionのマスクの割合を大きくすると悪化した。  
-> 公式実装で見つけられなかった。どこだろ。

### Revisiting BERT's Corruption Strategy
random replaceやsame replaceは、全てMASKにreplaceするときに比べてパフォーマンスが悪かった。

## 4. どうやって有効だと検証した？
24hBERTに以下の変化を加えたモデルを使用。
* RoBERTa's BPE tokenizer
* BERTの80-10-10のもとでのmaskingの代わりに100で[MASK]に置き換える

あまりはっきりと書いてないけどBLUEで検証したのかな。

## 5. 議論はある？
* ELECTRA-typeやseq2seq-typeでの最適maskingの割合はどのくらいだろう？
* 別の言語ではどう？
* 実験は複数のseedで行っていないので結果は怪しいかも。  
->three seedsでやったって書いてあるな


## 6. 次に読むべき論文は？
* SpanBERT: Improving Pre-training by Representing and Predicting Spans
* PMI-Masking: Principled masking of correlated spans
* How to Train BERT with an Academic Budget
## 7. 実装
https://github.com/princeton-nlp/dinkytrain