# TransTab: Learning Transferable Tabular Transformers Across Tables

* URL: https://arxiv.org/abs/2205.09328
* authors: Zifeng Wang, Jimeng Sun
* Submitted on 19 May 2022 (v1)

### 1. どんなもの？
Tableデータにおける新しいフレームワークのTransTab, which stands for **Trans**ferable **Trans**formers for **Tab**ular anlysisの提案.

### 2. 先行研究と比べるとどこが凄い？  
先行研究ではテーブルデータに対して機械学習や深層学習を用いる時に訓練データとテストデータで同じカラムを要求していた。しかしTransTabは特徴量をテキストに変換しそれをtransformerを利用してembeddingsに変換することで同じカラムにとらわれないこと、データを無駄にしないことを可能にした。

### 3. 技術や手法のキモはどこ？
* inputの特徴量をCategorical/Textual feature, Binary feature, Numerical featureで場合分ける。
  * Categorical/Textual feature  
  カラム名とvalueをつなげてtokenized and encoded
  * Binary feature  
  計算量やメモリー削減のためにbinaryが1のときのみtokenized and encoded
  * Numerical feature  
  tokenizerは数字の識別があまり良くないらしいのでカラム名のみをtokenized and encodedしたものにvalueをかける
そうして出来たembeddingsをnormalizationして、同じ空間に同じ線形層を適用し、$E = E_{c} \bigotimes E_{u}(=E_{u, col^2} * x_{u}) \bigotimes E_{b} \bigotimes E^{cls}$とする
* EをMulti-head self-attention layerに入力. その出力をtoken-wise gating layer(sigmoid)によって変換しそれぞれのtokenの重要度を求める。線形層を通したMulti-head self-attention layerの出力と、token-wise gating layerを通した出力とMulti-head self-attention layerの積の和をさらに線形層に通す。それを何回か繰り返す。
* Self-supervised Vertical-Partition Contrastive Learning  
本文の内容が全く分からないが、図を見るに、同じ行のセルをそれぞれembeddings上で近くなるように、異なる行のセルを遠ざけるように設定。
* Supervised Vertical-Partition Contrastive Learning  
本文の内容が全く分からないが、図を見るに、同じラベルをもつもののembeddingを近づけるようにして、異なるラベルを持つものを遠ざけるようにする

### 4. どうやって有効だと検証した？
dataset: clinical traial mortality prediction datasets and public datasets

1. How does TransTab perform compared with baselines under the vanilla supervised setting?  
  -> Yes(SOTA without pre-training)
2. How well does TransTab address incremental columns from a streeam of data?  
  -> Yes(データをいくつかに分割して学習させた)
3. How is the impact of TransTab learned from muliple tables drawn from the same domain in its oredctive ability?  
   -> Yes(データを50％重複するように分割し片方でpre-train, もう片方でfine-tuning)
4. Can TransTab be a zero-shot learner when pretrained on tables and infer on a new table?  
   -> Yes(データを三つに分解し、そのうち2つをpre-train、もう一つでZero-shotした)
5. Is the proposed vertical partition CL better than vanilla supervised pretraining and self-supervised CL?  
   -> Yes
### 5. 議論はある？


### 6. 次に読むべき論文は？
* Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models(tokenizerによる数字の識別の検証)
* Layer Normalization
* TabTransformer: Tabular Data Modeling Using Contextual Embeddings
* TabNet: Attentive Interpretable Tabular Learning
* Revisiting Deep Learning Models for Tabular Data

### 7. 実装
https://github.com/ryanwangzf/transtab