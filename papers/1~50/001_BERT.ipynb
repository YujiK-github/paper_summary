{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* URL: https://arxiv.org/abs/1810.04805\n",
    "* authors: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova\n",
    "* Submitted on 11 Oct 2018 (v1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. どんなもの？\n",
    "  新しい言語表現モデルであるBERT, which stands for **B**idirectional **E**ncoder **R**epresentations from **T**ransformersの紹介."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 先行研究と比べるとどこが凄い？\n",
    "* 先行研究であるGPTは文章の進む方向に対して一方向の学習をしているのに対して, BERTは文章の双方向の文脈を考慮するように学習させることで性能を向上させた.\n",
    "* 最低限の置き換えとfine-tuningで, 様々なタスクに対して対応することが出来る."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 技術や手法のキモはどこ？\n",
    "* MLM(Masked Language Model)  \n",
    "    文章の15％を以下のルールに従って置き換える  \n",
    "    * 80%: the [MASK] token\n",
    "    * 10%: random token\n",
    "    * 10%: unchanged token  \n",
    "    そして置き換えられたオリジナルの単語をCross Entropy Lossを用いて予測する。\n",
    "* NSP(Next Sentence Prediction)  \n",
    "    50%の確率で文章Bは実際に文章Aに続くもので、50%の確率で文章Bは適当に選ばれたものとする。このときに、文章が連続したものであるか否かをBCEを用いて予測する.\n",
    "* Use data\n",
    "    * BooksCorpus(800M words)\n",
    "    * English Wikipedia(2,500M words), which is extracted lists, tables, headers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. どうやって有効だと検証した？\n",
    "It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 議論はある？\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6. 次に読むべき論文は？\n",
    "* [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 実装\n",
    "https://github.com/google-research/bert"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
