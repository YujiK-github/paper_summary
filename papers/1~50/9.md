# Don't Stop Pretraining: Adapt Language Models to Domains and Tasks

* URL: https://arxiv.org/abs/2004.10964
* authors: Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A. Smith
* Submitted on 23 Apr 2020 (v1)

### 1. どんなもの？
originalのLMでpre-trainをした後でも、さらなるpre-trainingは有効かを示した論文.

### 2. 先行研究と比べるとどこが凄い？  
先行研究では, ドメインに特化したデータに対するcontinued pretrainingは有効だと示しているが、これは一度に一つの領域しか考慮しておらず、近年の言語モデルに比べて小さく多様性に乏しいコーパスを用いている言語モデルを使っている。その上, continued pretrainingの有用性がどの程度ラベル付きデータの量などの要因によって変化するのかが示されていなかった。そこで、4つのドメインと8つの分類タスクを行った。Task-adaptive pretraining(TAPT)の有用性は示されていたが、最近の言語モデルに対して使われていなかったので適用した。

### 3. 技術や手法のキモはどこ？
* Domain adaptive pretraining(DAPT)  
  RoBERTaのpretrainingの領域にないものに対する訓練.  
  新しく用意したデータセットとRoBERTaに学習させたデータセットでそれぞれthe top 10k most frequent unigramで語彙のオーバーラッピングを調べた。さらに分類タスクを行ったところ、RoBERTaのdomainが追加データと離れているほど結果が改善することが分かった。無関係なデータで追加学習するのは有害となる。
* Task adaptive pretraining(TAPT)
* DART + TAPT
  best
* Augmenting Training Data for Task-Adaptive pretraining  
  KNNと人間によるものとRANDoM -> 人間によるもの > KNN > RANDOM


### 4. どうやって有効だと検証した？
実験して比較.

### 5. 議論はある？

### 6. 次に読むべき論文は？

### 7. 実装
https://github.com/allenai/dont-stop-pretraining