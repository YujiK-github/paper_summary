{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* URL: https://arxiv.org/abs/1910.01108\n",
    "* authors: Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf\n",
    "* Submitted on 2 Oct 2019 (v1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. どんなもの？\n",
    "知識蒸留を利用した言語モデルのDistilBERT, which stands for a distilled version of BERTの提案."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 先行研究と比べるとどこが凄い？  \n",
    "BERTに比べてサイズを40％減らしながら言語モデルの性能を97%維持し、60%速くなっている."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 技術や手法のキモはどこ？\n",
    "pre-trainingの段階での知識蒸留.\n",
    "* Student model\n",
    "  BERTのthe token embeddings and the poolerを除去。層の数を半分にする。重みはteather modelの2つのうちの一つの層？で初期化する。gradient accumulationを利用してバッチサイズを増やし、Dynamic Maskingを用い、NSP Lossは用いなかった。\n",
    "* 知識蒸留のlossは  \n",
    "\n",
    "$$\n",
    "\\displaystyle L_{ce} = \\sum_{i}{t_{i} * \\log(s_{i})}\n",
    "$$\n",
    "\n",
    "ただし$t_{i}$はteather modelによって推測された確率, $s_{i}$はstudent modelによって推測された確率.  \n",
    "それぞれの確率はSoftmax-temperatureを使って  \n",
    "\n",
    "$$\n",
    "\\displaystyle p_{i} = \\frac{\\exp({z_{i}}/{T})}{\\sum_{j}\\exp(z_{j}/T)}\n",
    "$$ \n",
    "\n",
    "ただし$T$は出力の分布をならすための値で$z_{i}$はclass $i$ のモデルスコア.  teacherとstudentで同じ$T$の値を用いる.\n",
    "* 最終的なトレーニングのobjectiveは$L_{ce}$と$L_{mlm}$と$L_{cos}$の線形結合"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. どうやって有効だと検証した？\n",
    "* GLUE\n",
    "  fine-tuningなしで?BERTの97%の性能を持つことを示す\n",
    "* Downstream task \n",
    "  * IMDs \n",
    "  * SQuAD v1.1\n",
    "* Speed and inference size\n",
    "  CPU, batch_size 1で検証."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 議論はある？\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 次に読むべき論文は？\n",
    "* Distilling the Knowledge in a Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 7. 実装\n",
    "* https://github.com/huggingface/transformers\n",
    "* https://github.com/huggingface/swift-coreml-transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
