{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa: A Robustly Optimized BERT Pretraining Approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* URL: https://arxiv.org/abs/1907.11692\n",
    "* authors: Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov\n",
    "* Submitted on 26 Jul 2019"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. どんなもの？\n",
    "  新しい言語表現モデルであるRoBERTa, which stands for a **R**obustly **o**ptimized **BERT** **a**pproach の紹介."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 先行研究と比べるとどこが凄い？\n",
    "* 先行研究であるBERTの訓練方法の一部を変更し、さらにテクニックを加えて学習させ、性能を向上させたモデル。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 技術や手法のキモはどこ？\n",
    "* Dynamic Masking  \n",
    "  BERTでは、StaticなMaskingをしていた。これはエポックごとに同じ部分をマスクしないようにするため、訓練は10回重複したデータを用いており、効率が良くなかった。そこでモデルにセンテンスを渡す際にマスクのパターンを変更するDynamicなMaskingを行うことでより大きなデータやより多くの回数を訓練できるようになった。精度の改善は微かに上回るほどだったが、学習効率の面からDynamic Maskingを採用した。\n",
    "* FULL-SENTENCES without NSP loss  \n",
    "  BERTでは二つの隣接した文章が同じ文章からなるものか否かを予測していた。しかし、最近の研究ではNSP lossの必要性に疑問が呈され、実験した。いくつかのフォーマット間で比較実験した。その結果、結果的にはDOC-SENTENCEが一番良かったが、batch_sizeを一定にした方が実験の比較がしやすいのでFULL-SENTENCEを選んだ。\n",
    "    * SEGMENT-PAIR+NSP: BERTのオリジナル。二つのsegmentのpairは複数の自然な文章を含み、その長さは512token以下に抑えられている。\n",
    "    * SENTENCE-PAIR+NSP: 一つの長いドキュメント、もしくは異なるドキュメントから連続的にサンプルされた文章のペア。\n",
    "    * FULL-SENTENCE: あるドキュメント、もしくは異なるドキュメントから連続的にサンプルされたfull sentenceのペア。NSPは含まず。インプットする文章は複数のドキュメントをまたぎ、そのドキュメント間には[SEP]tokenが挿入された。\n",
    "    * DOC-SENTENCE: FULL-SENTENCEで、インプットする文章が複数のドキュメントをまたぐことを除いたもの。batch_sizeを動的に変化させる。\n",
    "\n",
    "* large mini-batches  \n",
    "increase large mini-batches(base:256 -> 8K)\n",
    "\n",
    "* a larger byte-level BPE  \n",
    "文字と単語レベルの間の表現の組み合わせ。これによって莫大な語彙を共通に出来る？BERTはCharacter-level BPEを用いていた。ほぼ変わらなかったが、a larger byte-level BPEがマイナーな場合でも使えると信じてこれにした。\n",
    "\n",
    "* use more data  \n",
    "BERT(16GB) -> RoBERTa(over 160GB)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. どうやって有効だと検証した？\n",
    "GLUE, SQuAD, RaceでSOTAを達成"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 議論はある？\n",
    "BPEのEncoderの詳細な比較"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 次に読むべき論文は？\n",
    "* [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 実装\n",
    "https://github.com/facebookresearch/fairseq"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
