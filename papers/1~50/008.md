# Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models

* URL: https://arxiv.org/abs/2005.00683
* authors: Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, Xiang Ren
* Submitted on 2 May 2020 (v1)

### 1. どんなもの？
pre-training language models(PTLMs)は実は数字の常識知識(numerical commonsence knowledge)に弱いという調査.

### 2. 先行研究と比べるとどこが凄い？  
先行研究ではPTLMsは常識をエンコードして文章の表現(embedding)を作り出すとされていた。しかし数字に対してはあまり効果的ではないということを見つけた.

### 3. 技術や手法のキモはどこ？
[MASK]を利用して[MASK]に当てはまる確率が最も大きいものをモデルによる常識だと見なし、実験する.

### 4. どうやって有効だと検証した？
NumerSenceというdatasetをつくって検証した. NumerSenceはOpen Mind Common Sense(OMCS)から{"zero", "one", ... , "ten"}の12個の数字のどれかが含まれている文章を抜きだし、その文章をrefineさせ、全てのアノテーターによって受け入れられた文章のみを採用した。何書いてるか分からない??adversarial dataも追加.

* Zerp-shot setting
* distant supervision setting
  質問と似た分野のデータセットでfine-tuningする.    
BERTとRoBERTaを使う  
hit@1, 2, 3accuracy(正しい順番でランク付けされた単語のパーセンテージ)  
Zero-shot settingに比べてdistant supervision settingは性能が良かったものの、人間に比べると遙かに劣った結果になった。  
Open Questionだとfine-tuningなしでわずか15%程度の正解率  

### 5. 議論はある？
* 何で数字に弱いかの分析

### 6. 次に読むべき論文は？

### 7. 実装
No official code found