{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Batch Memory for Embedding Learning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* URL: https://arxiv.org/abs/1912.06798\n",
    "* authors: Xun Wang, Haozhi Zhang, Weilin Huang, Matthew R. Scott\n",
    "* Submitted on 14 Dec 2019 (v1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. どんなもの？\n",
    "XBM mechanism, where stands for Cross-Batch Memoryの提案。 \"Slow drift phenomena\"を利用することで過去のイテレーションのembeddingを記憶しハードネガティブなペアを見つけることができる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 先行研究と比べるとどこが凄い？  \n",
    "それぞれのmini-batch内でネガティブなものを選んでいた先行研究と比べて、複数のmini-batchから選べるようにすることで精度を上昇させることができた。Memory消費もほぼ0に近い。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 技術や手法のキモはどこ？\n",
    "### Slow drift Phenomena\n",
    "過去のmini-batchのembeddingを利用するために、異なるイテレーションで訓練されたfeatureがどれだけ変わるかを以下の式で表した。t-th iteration with $\\Delta t$のときのxのthe feature driftは\n",
    "\n",
    "$$\n",
    "D(x, t;\\Delta t) = ||f(\\mathbf{x};\\mathbf{\\theta}^{t}-f(\\mathbf{x};\\mathbf{\\theta}^{t-\\Delta t}))||^2_2 \\\\\n",
    "\\text{ , where } f(\\cdot;\\theta)\\text{ projects a data point }x_{i}\\text{into a }D\\text{-dimentional unit hyper-sphere}\n",
    "$$\n",
    "\n",
    "GoogleNetをContrastive lossで0から訓練し異なるiterationのときのthe feature driftの平均を計算する。iterationが小さいとき(e.g.10)には小さいが、大きいときには(e.g. 100 and 1000)早い段階で劇的に変化する。しかし3K iterationほどで安定する。さらに学習率を下げると変化の度合いも小さくなる。このような減少をslow driftとした。\n",
    "\n",
    "### XBM\n",
    "the feature driftはepochが早い段階では大きいのでwarmupとして1k iteration訓練する。その後にmini-batchのembeddingとlabelをqueueに追加し、dequeueする。??mini-batchのそれぞれのインスタンスがメモリーの中の全てのインスタンスと比較されてもっとも情報量の多いペアを見つけることができる。??\n",
    "以下の実験でqueueのmemory ratio(メモリーサイズ/全体の訓練データのサイズ)は1％程度で飽和することが分かった。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. どうやって有効だと検証した？\n",
    "Stanford Online Products(SOP), In-shop Clothes Retrieval(In-shop), PKU VehicleID(VehicleID)の3つのdatasetのrecall@kでSOTA。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 議論はある？\n",
    "mini-batchのそれぞれのインスタンスがメモリーの中の全てのインスタンスと比較されてもっとも情報量の多いペアを見つけることができる。のプロセスがいまいち分からない。  \n",
    "->普通に全部とコサイン類似度を比較しているのかな。\n",
    "以下公式実装より"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# trainer.py\n",
    "images = images.to(device)\n",
    "targets = targets.to(device)\n",
    "feats = model(images)\n",
    "\n",
    "if cfg.XBM.ENABLE and iteration > cfg.XBM.START_ITERATION:\n",
    "    xbm.enqueue_dequeue(feats.detach(), targets.detach())\n",
    "\n",
    "loss = criterion(feats, targets, feats, targets)\n",
    "log_info[\"batch_loss\"] = loss.item()\n",
    "\n",
    "if cfg.XBM.ENABLE and iteration > cfg.XBM.START_ITERATION:\n",
    "    xbm_feats, xbm_targets = xbm.get()\n",
    "    xbm_loss = criterion(feats, targets, xbm_feats, xbm_targets)\n",
    "    log_info[\"xbm_loss\"] = xbm_loss.item()\n",
    "    loss = loss + cfg.XBM.WEIGHT * xbm_loss\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Contrastive_loss.py\n",
    "@LOSS.register(\"contrastive_loss\")\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = 0.5\n",
    "\n",
    "    def forward(self, inputs_col, targets_col, inputs_row, target_row):\n",
    "\n",
    "        n = inputs_col.size(0)\n",
    "        # Compute similarity matrix\n",
    "        sim_mat = torch.matmul(inputs_col, inputs_row.t())\n",
    "        epsilon = 1e-5\n",
    "        loss = list()\n",
    "\n",
    "        neg_count = list()\n",
    "        for i in range(n):\n",
    "            pos_pair_ = torch.masked_select(sim_mat[i], targets_col[i] == target_row)\n",
    "            pos_pair_ = torch.masked_select(pos_pair_, pos_pair_ < 1 - epsilon)\n",
    "            neg_pair_ = torch.masked_select(sim_mat[i], targets_col[i] != target_row)\n",
    "\n",
    "            neg_pair = torch.masked_select(neg_pair_, neg_pair_ > self.margin)\n",
    "\n",
    "            pos_loss = torch.sum(-pos_pair_ + 1)\n",
    "            if len(neg_pair) > 0:\n",
    "                neg_loss = torch.sum(neg_pair)\n",
    "                neg_count.append(len(neg_pair))\n",
    "            else:\n",
    "                neg_loss = 0\n",
    "\n",
    "            loss.append(pos_loss + neg_loss)\n",
    "        if inputs_col.shape[0] == inputs_row.shape[0]:\n",
    "            prefix = \"batch_\"\n",
    "        else:\n",
    "            prefix = \"memory_\"\n",
    "        if len(neg_count) != 0:\n",
    "            log_info[f\"{prefix}average_neg\"] = sum(neg_count) / len(neg_count)\n",
    "        else:\n",
    "            log_info[f\"{prefix}average_neg\"] = 0\n",
    "        log_info[f\"{prefix}non_zero\"] = len(neg_count)\n",
    "        loss = sum(loss) / n  # / all_targets.shape[1]\n",
    "        return loss\n",
    "\n",
    "\n",
    "# xbm.py\n",
    "class XBM:\n",
    "    def __init__(self, cfg):\n",
    "        self.K = cfg.XBM.SIZE\n",
    "        self.feats = torch.zeros(self.K, 128).cuda()\n",
    "        self.targets = torch.zeros(self.K, dtype=torch.long).cuda()\n",
    "        self.ptr = 0\n",
    "\n",
    "    @property\n",
    "    def is_full(self):\n",
    "        return self.targets[-1].item() != 0\n",
    "\n",
    "    def get(self):\n",
    "        if self.is_full:\n",
    "            return self.feats, self.targets\n",
    "        else:\n",
    "            return self.feats[:self.ptr], self.targets[:self.ptr]\n",
    "\n",
    "    def enqueue_dequeue(self, feats, targets):\n",
    "        q_size = len(targets)\n",
    "        if self.ptr + q_size > self.K:\n",
    "            self.feats[-q_size:] = feats\n",
    "            self.targets[-q_size:] = targets\n",
    "            self.ptr = 0\n",
    "        else:\n",
    "            self.feats[self.ptr: self.ptr + q_size] = feats\n",
    "            self.targets[self.ptr: self.ptr + q_size] = targets\n",
    "            self.ptr += q_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 次に読むべき論文は？\n",
    "* Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning\n",
    "\n",
    "## 7. 実装\n",
    "https://github.com/MalongTech/research-xbm"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
