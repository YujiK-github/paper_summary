{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't Stop Pretraining: Adapt Language Models to Domains and Tasks\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* URL: https://arxiv.org/abs/2004.10964\n",
    "* authors: Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A. Smith\n",
    "* Submitted on 23 Apr 2020 (v1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. どんなもの？\n",
    "originalのLMでpre-trainをした後でも、さらなるpre-trainingは有効かを示した論文."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. 先行研究と比べるとどこが凄い？  \n",
    "先行研究では, ドメインに特化したデータに対するcontinued pretrainingは有効だと示しているが、これは一度に一つの領域しか考慮しておらず、近年の言語モデルに比べて小さく多様性に乏しいコーパスを用いている言語モデルを使っている。その上, continued pretrainingの有用性がどの程度ラベル付きデータの量などの要因によって変化するのかが示されていなかった。そこで、4つのドメインと8つの分類タスクを行った。Task-adaptive pretraining(TAPT)の有用性は示されていたが、最近の言語モデルに対して使われていなかったので適用した。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 技術や手法のキモはどこ？\n",
    "* Domain adaptive pretraining(DAPT)  \n",
    "  RoBERTaのpretrainingの領域にないものに対する訓練.  \n",
    "  新しく用意したデータセットとRoBERTaに学習させたデータセットでそれぞれthe top 10k most frequent unigramで語彙のオーバーラッピングを調べた。さらに分類タスクを行ったところ、RoBERTaのdomainが追加データと離れているほど結果が改善することが分かった。無関係なデータで追加学習するのは有害となる。\n",
    "* Task adaptive pretraining(TAPT)\n",
    "* DART + TAPT\n",
    "  best\n",
    "* Augmenting Training Data for Task-Adaptive pretraining  \n",
    "  KNNと人間によるものとRANDoM -> 人間によるもの > KNN > RANDOM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. どうやって有効だと検証した？\n",
    "実験して比較."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 議論はある？\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 次に読むべき論文は？\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 実装\n",
    "https://github.com/allenai/dont-stop-pretraining"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
