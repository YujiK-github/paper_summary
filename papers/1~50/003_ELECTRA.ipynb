{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* URL: https://arxiv.org/abs/2003.10555\n",
    "* authers: Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning\n",
    "* Submitted on 23 Mar 2020"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. どんなもの？\n",
    "  新しい言語表現モデルであるELECTRA,  which stands for **E**fficiently **L**earning an **E**ncoder that **C**lassifies **T**oken **R**eplacements **A**ccurately の紹介."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 先行研究と比べるとどこが凄い？\n",
    "* 先行研究であるBERTやRoBERTaのようなMasked Language Modeling pre-trained methodとは異なってreplacing token detectionを導入することによって、訓練効率を上昇させた。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 技術や手法のキモはどこ？\n",
    "* Replacing token detection  \n",
    "モデルをgeneratorとdiscriminatorの2つに分けた。inputに一部[MASK]をして、そのtokenをgeneratorによって最もそれらしい(尤度が最大の)tokenに置き換え(generatorが[MASK]前と同じtokenを置き換えたらそのままのtokenを出すようにした)、discriminatorに入力。そこでどのtokenが置き換えられているのかを予測するようにして学習させた。generator部分ではback-propagateは行わない。pre-trainedが終わったらgenerator部分は使わず、discriminator部分のみでdownstream taskに対してfine-tuningを行う。損失関数は以下の通り。詳しいことは論文参照。\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    " L_{MLM}(x, \\theta_{G}) = E\\left(\\sum_{i \\in m}-\\log_{p_{G}}(x_{i}|x^{masked})\\right)\\\\\n",
    " L_{Disc}(x, \\theta_{D}) = E\\left(\\sum_{t=1}^{n}-1(x_{t}^{corrupt}=x_{t})\\log(D(x^{corrupt}, t)) -1(x_{t}^{corrupt} \\ne x_{t})\\log(1-D(1-D(x^{corrupt}, t)))\\right)\\\\\n",
    " \\min_{\\theta_{G}, \\theta_{D}}\\sum_{x\\in X} L_{MLM}(x, \\theta_{G}) + \\lambda L_{Disc}{x, \\theta_{D}}\\\\\n",
    " D = softmax\\\\\n",
    " p = simgoid\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* Weight Sharing  \n",
    "訓練効率を改善させるためにgeneratorとdiscriminetorでweightを共有することにした。実験ではdiscriminetorよりもlayer数が少なく、サイズが1/2~1/4のgeneratorを用いると精度が上昇したので、embeddings(token and positional embeddings)のみを共有することにした。\n",
    "\n",
    "* 小さいモデルでも大きいモデルでも性能が高かった。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. どうやって有効だと検証した？\n",
    "GLUEとSQuADでモデルのサイズや訓練回数、評価指標に対する評価を10のseedのmedianを取って比較。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 議論はある？\n",
    "* ELECTRAを多言語にするとどうなる？\n",
    "* ELECTRAのgeneratorで、MLMではなくauto-regressiveにしたらどうなるのか？\n",
    "* 大量のデータを使うとどうなる？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6. 次に読むべき論文は？\n",
    "* [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)\n",
    "* MASS: auto-regressive\n",
    "* UniLM: auto-regressive"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 実装\n",
    "https://github.com/google-research/electra"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
