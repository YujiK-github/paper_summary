# DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter

* URL: https://arxiv.org/abs/1910.01108
* authors: Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf
* Submitted on 2 Oct 2019 (v1)

### 1. どんなもの？
知識蒸留を利用した言語モデルのDistilBERT, which stands for a distilled version of BERTの提案.

### 2. 先行研究と比べるとどこが凄い？  
BERTに比べてサイズを40％減らしながら言語モデルの性能を97%維持し、60%速くなっている.

### 3. 技術や手法のキモはどこ？
pre-trainingの段階での知識蒸留.
* Student model
  BERTのthe token embeddings and the poolerを除去。層の数を半分にする。重みはteather modelの2つのうちの一つの層？で初期化する。gradient accumulationを利用してバッチサイズを増やし、Dynamic Maskingを用い、NSP Lossは用いなかった。
* 知識蒸留のlossは  

$$
\displaystyle L_{ce} = \sum_{i}{t_{i} * \log(s_{i})}
$$

ただし$t_{i}$はteather modelによって推測された確率, $s_{i}$はstudent modelによって推測された確率.  
それぞれの確率はSoftmax-temperatureを使って  

$$
\displaystyle p_{i} = \frac{\exp({z_{i}}/{T})}{\sum_{j}\exp(z_{j}/T)}
$$ 

ただし$T$は出力の分布をならすための値で$z_{i}$はclass$i$のモデルスコア.teacherとstudentで同じ$T$の値を用いる.
* 最終的なトレーニングのobjectiveは$L_{ce}$と$L_{mlm}$と$L_{cos}$の線形結合

### 4. どうやって有効だと検証した？
* GLUE
  fine-tuningなしで?BERTの97%の性能を持つことを示す
* Downstream task 
  * IMDs 
  * SQuAD v1.1
* Speed and inference size
  CPU, batch_size 1で検証.

### 5. 議論はある？

### 6. 次に読むべき論文は？
* Distilling the Knowledge in a Neural Network

### 7. 実装
* https://github.com/huggingface/transformers
* https://github.com/huggingface/swift-coreml-transformers