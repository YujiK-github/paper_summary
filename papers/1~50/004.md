# OCR-free Document Understanding Transformer

* URL: https://arxiv.org/abs/2111.15664
* authors: Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park
* Submitted on 30 Nov 2021 (v1)

### 1. どんなもの？
新たなモデルのDonut, which stands for **Do**cume**n**t **u**nderstanding **t**ranformer の提案

### 2. 先行研究と比べるとどこが凄い？
先行研究では、OCRを用いて情報を抽出し、その結果を用いてBERTやLMに入力しタスクに取り組んでいた。しかしDonutはEnd2Endで行う。OCRは計算コストが高く、ドキュメントの異なる言語に対して柔軟ではない。さらに後続のモデルに悪影響を与えるのでそれに対処しようとした。

### 3. 技術や手法のキモはどこ？
* EncoderにはSwin Transformerを使用。
* DecoderのarchitectureとしてBARTを使用。Mulit-lang weights.
* Outputはspecial tokenをつくって<START_\*><END_\*>の間にあるものを抽出したものとする
* pre-train  
画像の中の全ての文章を順番通りに(基本的に上から下、左から右)全て読ませるように訓練する。画像とそれまでの文章を基に次のトークンを予測する with CrossEntropyLoss.
    * SynthDoG (**Synth**etic **Do**cument **G**enerator): 多言語のデータでも使えるように作った生成器。ImageNetの画像を背景に一定のルールのもとで階層構造を作り、Wikipediaから文章を挿入したもの。後処理として色変えたり、ガウスノイズが入ったり。
  
* fine-tuning  
どのように読ませるか(つまりどのように出力させるか)を訓練する

### 4. どうやって有効だと検証した？
* Document Classification  
  他の一般的なモデルのようなclassifierを用いてsoftmax, ではなく前述のようなoutputの出力を指定した結果を用いる
  * RVL_CDIP: dataset  
  -> SOTA and Speed up
* Document Information Extraction  
  階層関係やグループをうまくとらえられているかどうかのテスト. 評価指標はfield-level F1 scoreとTree Edit Distance(TED).
  * CORD
  * Ticket
  * Bussiness Card
  * Receipt  
  -> SOTA and 安定した結果
* Document Visual Question and Answering(DocVQA)  
画像と質問文から答えを導くタスク。
  * DocVQA  
  評価指標はANLS(Average Normalize Levenshtein Similarity)  
  -> 一部では敗北、しかし手書き文字に対してはSOTA。

### 5. 議論はある？
* 画像のサイズを増やしたときに急速に計算量が増加する。ここではoriginalのtransformerを用いているが、a efficient attention mechanismを使うと避けられるかもしれない？

### 6. 次に読むべき論文は？
* [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)
* [DePlot: One-shot visual language reasoning by plot-to-table translation](https://arxiv.org/abs/2212.10505)

### 7. 実装
https://github.com/clovaai/donut