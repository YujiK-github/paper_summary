{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost: A Scalable Tree Boosting System\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* URL: https://arxiv.org/abs/1603.02754\n",
    "* Tianqi Chen, Carlos Guestrin\n",
    "* Submitted on 9 Mar 2016 (v1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. どんなもの？\n",
    "XGBoostの提案。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 先行研究と比べるとどこが凄い？  \n",
    "* parallel tree boostingについての先行研究では、out-of-core computation, cashe-aware and sparsity-aware learingは研究されていなかった。\n",
    "* a regularized learning objectiveの提案により改良をもたらした。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 技術や手法のキモはどこ？\n",
    "* a novel tree algorithm if for handling sparse data\n",
    "* a theoretically justified weighted quantile sktch procedure\n",
    "* parallel and distributed computing\n",
    "* exploit out-of-core computation\n",
    "* Regurlarized Learning Objective  \n",
    "  全然分からん\n",
    "* Shrinkage  \n",
    "  over-fittingを防ぐためのテクニック\n",
    "* Column(feature) subsampling  \n",
    "  over-fittingを防ぐためのテクニック"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. どうやって有効だと検証した？\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 議論はある？\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 次に読むべき論文は？\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 実装\n",
    "https://github.com/dmlc/xgboost"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
