# papers
一覧
## 1~50
001. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
002. RoBERTa: A Robustly Optimized BERT Pretraining Approach
003. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators
004. OCR-free Document Understanding Transformer
005. DePlot: One-shot visual language reasoning by plot-to-table translation
006. TransTab: Learning Transferable Tabular Transformers Across Tables 
007. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
008. Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models
009. Don't Stop Pretraining: Adapt Language Models to Domains and Tasks
010. Cross-Batch Memory for Embedding Learning
011. Attention Is All You Need 
012. Should You Mask 15% in Masked Language Modeling?
013. A Simple Framework for Contrastive Learning of Visual Representations
014. XGBoost: A Scalable Tree Boosting System(未完成)
015. Generative Adversarial Networks(未完成)
016. DeBERTa: Decoding-enhanced BERT with Disentangled Attention(未完成)
017. Deep State Space Models for Time Series Forecasting
018. SphereFace: Deep Hypersphere Embedding for Face Recognition(未完成)
019. DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks
020. Efficiently Modeling Long Sequences with Structured State Spaces(未完成)