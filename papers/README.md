# papers
一覧
## 1~50
1. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
2. RoBERTa: A Robustly Optimized BERT Pretraining Approach
3. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators
4. OCR-free Document Understanding Transformer
5. DePlot: One-shot visual language reasoning by plot-to-table translation
6. TransTab: Learning Transferable Tabular Transformers Across Tables 
7. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
8. Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models
9. Don't Stop Pretraining: Adapt Language Models to Domains and Tasks
10. Cross-Batch Memory for Embedding Learning
11. Attention Is All You Need 
12. Should You Mask 15% in Masked Language Modeling?
13. A Simple Framework for Contrastive Learning of Visual Representations
14. XGBoost: A Scalable Tree Boosting System
15. Generative Adversarial Networks
16. DeBERTa: Decoding-enhanced BERT with Disentangled Attention