# papers
一覧
## 1~50
001. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
002. RoBERTa: A Robustly Optimized BERT Pretraining Approach
003. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators
004. OCR-free Document Understanding Transformer
005. DePlot: One-shot visual language reasoning by plot-to-table translation
006. TransTab: Learning Transferable Tabular Transformers Across Tables 
007. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
008. Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models
009. Don't Stop Pretraining: Adapt Language Models to Domains and Tasks
010. Cross-Batch Memory for Embedding Learning
011. Attention Is All You Need 
012. Should You Mask 15% in Masked Language Modeling?
013. A Simple Framework for Contrastive Learning of Visual Representations
014. XGBoost: A Scalable Tree Boosting System
015. Generative Adversarial Networks
016. DeBERTa: Decoding-enhanced BERT with Disentangled Attention